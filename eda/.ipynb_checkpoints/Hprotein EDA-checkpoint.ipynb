{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farrar/py3.6.5/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import cv2\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, Conv2D, MaxPooling2D, BatchNormalization, Concatenate, ReLU, LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "TRAIN_PATH = '../stage1_data/train'\n",
    "LABEL_PATH = '../stage1_labels/train.csv'\n",
    "COLORS = ['red','green', 'blue', 'yellow']\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 2\n",
    "SHAPE = (192, 192, 4)\n",
    "THRESHOLD = 0.05\n",
    "\n",
    "\n",
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000c99ba-bba4-11e8-b2b9-ac1f6b6435d0', '001bcdd2-bbb2-11e8-b2ba-ac1f6b6435d0', '001838f8-bbca-11e8-b2bc-ac1f6b6435d0', '0020af02-bbba-11e8-b2ba-ac1f6b6435d0', '002daad6-bbc9-11e8-b2bc-ac1f6b6435d0']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# get a list of unique specimen ids\n",
    "# -----------------------------------------\n",
    "def get_specimen_ids(path):\n",
    "\n",
    "    # get a list of all the images\n",
    "    file_list = file_io.list_directory(path)\n",
    "    \n",
    "    # truncate the file names to make a specimen id\n",
    "    specimen_ids = [f[:36] for f in file_list]\n",
    "    \n",
    "    # eliminate duplicates\n",
    "    specimen_ids = list(set(specimen_ids))\n",
    "    \n",
    "    return specimen_ids\n",
    "\n",
    "# unit test\n",
    "specimen_list = get_specimen_ids(TRAIN_PATH)\n",
    "print(specimen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../stage1_data/train/000c99ba-bba4-11e8-b2b9-ac1f6b6435d0_red.png\n"
     ]
    }
   ],
   "source": [
    "def get_image_fname(specimen_id, color, lo_res=True):\n",
    "\n",
    "    # construct filename\n",
    "    if lo_res:\n",
    "        fname = TRAIN_PATH + '/' + specimen_id + '_' + color + '.png'\n",
    "    else:\n",
    "        fname = TRAIN_PATH + '/' + specimen_id + '_' + color + '.tif'\n",
    "        \n",
    "    return fname\n",
    "\n",
    "# unit test\n",
    "s = '000c99ba-bba4-11e8-b2b9-ac1f6b6435d0'\n",
    "f = get_image_fname(s, 'red')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_single_image(specimen_id, color, lo_res=True):\\n    \\n    # get image file name\\n    fname = get_image_fname(specimen_id, color, lo_res)\\n    \\n    # read image as a 1-channel image\\n    image = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\\n    \\n    return image\\n\\n# unit test\\ns = '000c99ba-bba4-11e8-b2b9-ac1f6b6435d0'\\nr = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\\nb = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\\ng = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\\ny = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\\n\\nr[:,:,0] = get_single_image(s, COLORS[0])\\ng[:,:,1] = get_single_image(s, COLORS[1])\\nb[:,:,2] = get_single_image(s, COLORS[2])\\ny[:,:,0] = get_single_image(s, COLORS[3])\\n\\nfig, axarr = plt.subplots(nrows=1, ncols=4, figsize=(15,15))\\naxarr[0].imshow(r)\\naxarr[1].imshow(g)\\naxarr[2].imshow(b)\\naxarr[3].imshow(y)\\nprint(COLORS)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# get a single image\n",
    "# -----------------------------------------\n",
    "'''\n",
    "def get_single_image(specimen_id, color, lo_res=True):\n",
    "    \n",
    "    # get image file name\n",
    "    fname = get_image_fname(specimen_id, color, lo_res)\n",
    "    \n",
    "    # read image as a 1-channel image\n",
    "    image = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# unit test\n",
    "s = '000c99ba-bba4-11e8-b2b9-ac1f6b6435d0'\n",
    "r = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "b = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "g = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "y = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "r[:,:,0] = get_single_image(s, COLORS[0])\n",
    "g[:,:,1] = get_single_image(s, COLORS[1])\n",
    "b[:,:,2] = get_single_image(s, COLORS[2])\n",
    "y[:,:,0] = get_single_image(s, COLORS[3])\n",
    "\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=4, figsize=(15,15))\n",
    "axarr[0].imshow(r)\n",
    "axarr[1].imshow(g)\n",
    "axarr[2].imshow(b)\n",
    "axarr[3].imshow(y)\n",
    "print(COLORS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_stacked_image(specimen_id, lo_res=True):\\n\\n    # create a numpy array to place the 1-channel images into\\n    image = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 4), dtype=np.uint8)\\n                \\n    for n, color in enumerate(COLORS):\\n\\n        # get a single image\\n        i = get_single_image(specimen_id, color, lo_res)\\n        \\n        # store it a channel\\n        image[:, :, n] = i\\n        \\n    return image\\n        \\n# unit test\\ns = '000c99ba-bba4-11e8-b2b9-ac1f6b6435d0'\\ni = get_stacked_image(s)\\nplt.imshow(i[:,:,0:3])\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# get a stacked (4-channel) image\n",
    "# -----------------------------------------\n",
    "'''\n",
    "def get_stacked_image(specimen_id, lo_res=True):\n",
    "\n",
    "    # create a numpy array to place the 1-channel images into\n",
    "    image = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 4), dtype=np.uint8)\n",
    "                \n",
    "    for n, color in enumerate(COLORS):\n",
    "\n",
    "        # get a single image\n",
    "        i = get_single_image(specimen_id, color, lo_res)\n",
    "        \n",
    "        # store it a channel\n",
    "        image[:, :, n] = i\n",
    "        \n",
    "    return image\n",
    "        \n",
    "# unit test\n",
    "s = '000c99ba-bba4-11e8-b2b9-ac1f6b6435d0'\n",
    "i = get_stacked_image(s)\n",
    "plt.imshow(i[:,:,0:3])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv(LABEL_PATH)\n",
    "train_labels.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    [7, 1, 2, 0]\n",
      "Name: Target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "selection_list = ['000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0', s, '001838f8-bbca-11e8-b2bc-ac1f6b6435d0']\n",
    "subset = train_labels.loc[train_labels['Id'].isin(selection_list)]\n",
    "subset.head()\n",
    "split_labels = (subset.loc[subset['Id'] == '000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0'])['Target'].str.split(' ')\n",
    "print (split_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Keras style run time data generator\n",
    "# ---------------------------------------\n",
    "class HproteinDataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    # ---------------------------------------------\n",
    "    # Required function to initialize the class\n",
    "    # ---------------------------------------------\n",
    "    def __init__(self, \n",
    "                 specimen_ids, \n",
    "                 labels, \n",
    "                 batch_size=BATCH_SIZE, \n",
    "                 shape=SHAPE, \n",
    "                 shuffle=False, \n",
    "                 use_cache=False, \n",
    "                 augment=False):\n",
    "       \n",
    "        self.specimen_ids = specimen_ids       # list of features\n",
    "        self.labels = labels                   # list of labels\n",
    "        self.batch_size = batch_size           # batch size\n",
    "        self.shape = shape                     # shape of features\n",
    "        self.shuffle = shuffle                 # boolean for shuffle\n",
    "        self.use_cache = use_cache             # boolean for use of cache\n",
    "        self.augment = augment                 # boolean for image augmentation\n",
    "        \n",
    "    # -------------------------------------------------------\n",
    "    # Required function to determine the number of batches\n",
    "    # -------------------------------------------------------\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.specimen_ids) / float(self.batch_size)))\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # Required function to get a batch\n",
    "    # -------------------------------------------------------\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # get the list of specimen ids for this batch\n",
    "        specimen_ids = self.specimen_ids[self.batch_size*index:self.batch_size*(index + 1)]\n",
    "                \n",
    "        # create a zeroed out numpy array to load the batch into\n",
    "        feature_batch = np.zeros((len(specimen_ids), self.shape[0], self.shape[1], self.shape[2]))\n",
    "        \n",
    "        # load a batch of labels\n",
    "        label_batch = self.labels[self.batch_size*index:self.batch_size*(index + 1)]\n",
    "        \n",
    "        # load a batch of images\n",
    "        if self.use_cache:\n",
    "            print(\"Error: use_cache not implemented!\")\n",
    "        else:\n",
    "            for i, specimen_id in enumerate(specimen_ids):\n",
    "                print(feature_batch[i].shape)\n",
    "                feature_batch[i] = self.get_stacked_image(specimen_id)\n",
    "            \n",
    "        # augment images if desired\n",
    "        if self.augment:\n",
    "            print(\"Error: Image augmentation not implemented!\")\n",
    "            \n",
    "        return feature_batch, label_batch\n",
    "            \n",
    "            \n",
    "    # -----------------------------------------\n",
    "    # get a single image\n",
    "    # -----------------------------------------\n",
    "    def get_single_image(self, specimen_id, color, lo_res=True):\n",
    "\n",
    "        # get image file name\n",
    "        fname = get_image_fname(specimen_id, color, lo_res)\n",
    "\n",
    "        # read image as a 1-channel image\n",
    "        image = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        image = cv2.resize(image, (self.shape[0], self.shape[1]))\n",
    "\n",
    "        return image\n",
    "    \n",
    "            \n",
    "    # -----------------------------------------\n",
    "    # get a stacked (4-channel) image\n",
    "    # -----------------------------------------\n",
    "    def get_stacked_image(self, specimen_id, lo_res=True):\n",
    "\n",
    "        # create a numpy array to place the 1-channel images into\n",
    "        image = np.zeros((self.shape[0], self.shape[1], 4), dtype=np.uint8)\n",
    "\n",
    "        for n, color in enumerate(COLORS):\n",
    "\n",
    "            # get a single image\n",
    "            i = self.get_single_image(specimen_id, color, lo_res)\n",
    "\n",
    "            # store it a channel\n",
    "            image[:, :, n] = i\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specimen Id: 000c99ba-bba4-11e8-b2b9-ac1f6b6435d0 : Labels: [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Specimen Id: 001bcdd2-bbb2-11e8-b2ba-ac1f6b6435d0 : Labels: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Specimen Id: 001838f8-bbca-11e8-b2bc-ac1f6b6435d0 : Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Specimen Id: 0020af02-bbba-11e8-b2ba-ac1f6b6435d0 : Labels: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "Specimen Id: 002daad6-bbc9-11e8-b2bc-ac1f6b6435d0 : Labels: [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# get the available specimen ids and corresponding labels\n",
    "# -----------------------------------------------------------\n",
    "def get_train_data(train_path, label_path):\n",
    "    \n",
    "    # get the list of specimen ids\n",
    "    specimen_ids = get_specimen_ids(train_path)\n",
    "    \n",
    "    # get the labels for all specimen_ids\n",
    "    label_data = pd.read_csv(label_path)\n",
    "    \n",
    "    # get the subset of labels that match the specimen images that are on TRAIN_PATH\n",
    "    labels_subset = label_data.loc[label_data['Id'].isin(specimen_ids)]\n",
    "    \n",
    "    #\n",
    "    # convert labels to trainer format\n",
    "    #\n",
    "    \n",
    "    # set up the list that will contain the list of encoded labels for each specimen id\n",
    "    labels = []\n",
    "    \n",
    "    # loop through each specimen_id\n",
    "    for specimen_id in specimen_ids:\n",
    "        \n",
    "        # split the space separated multi-label into a list of individual labels\n",
    "        split_labels = (labels_subset.loc[labels_subset['Id'] == specimen_id])['Target'].str.split(' ')\n",
    "\n",
    "        # set up a numpy array to receive the encoded label\n",
    "        l = np.zeros(28, dtype=np.uint8)\n",
    "\n",
    "        # turn on the positive columns in the labels array\n",
    "        for label in split_labels:\n",
    "            l[np.uint8(label)] = 1\n",
    "        \n",
    "        labels.append(l)\n",
    "        \n",
    "    return specimen_ids, labels\n",
    "\n",
    "# unit test \n",
    "specimen_ids, labels = get_train_data(TRAIN_PATH, LABEL_PATH)\n",
    "for sid, l in zip(specimen_ids, labels):\n",
    "    print('Specimen Id: {} : Labels: {}'.format(sid, l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473683\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# calculate the f1 statistic\n",
    "# --------------------------------\n",
    "def f1(y_true, y_pred):\n",
    "    \n",
    "    #y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    \n",
    "    return K.mean(f1)\n",
    "\n",
    "# unit test\n",
    "y_true = K.variable(np.ones(10, np.uint8))\n",
    "y_pred = np.ones(10, np.uint8)\n",
    "y_pred[0] = 0\n",
    "y_pred = K.variable(y_pred)\n",
    "\n",
    "ftest = f1(y_true, y_pred)\n",
    "#ftest = tf.constant(5)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "print (sess.run(ftest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052631676\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# calculate the f1 loss\n",
    "# --------------------------------\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "\n",
    "    f = f1(y_true, y_pred)\n",
    "    \n",
    "    return 1 - K.mean(f)\n",
    "\n",
    "# unit test\n",
    "y_true = K.variable(np.ones(10, np.uint8))\n",
    "y_pred = np.ones(10, np.uint8)\n",
    "y_pred[0] = 0\n",
    "y_pred = K.variable(y_pred)\n",
    "\n",
    "ftest = f1_loss(y_true, y_pred)\n",
    "#ftest = tf.constant(5)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "print (sess.run(ftest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 192, 192, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 192, 192, 4)  16          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 190, 190, 8)  296         batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 190, 190, 8)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 190, 190, 8)  32          re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 188, 188, 8)  584         batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 188, 188, 8)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 188, 188, 8)  32          re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 186, 186, 16) 1168        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 186, 186, 16) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 186, 186, 16) 64          re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 93, 93, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 93, 93, 16)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 93, 93, 16)   2320        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 93, 93, 16)   6416        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 93, 93, 16)   12560       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 93, 93, 16)   272         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 93, 93, 16)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 93, 93, 16)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 93, 93, 16)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 93, 93, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 93, 93, 64)   0           re_lu_4[0][0]                    \n",
      "                                                                 re_lu_5[0][0]                    \n",
      "                                                                 re_lu_6[0][0]                    \n",
      "                                                                 re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 93, 93, 64)   256         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 46, 46, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 46, 46, 64)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 44, 44, 32)   18464       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 44, 44, 32)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 44, 44, 32)   128         re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 22, 22, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 22, 22, 32)   0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 20, 20, 64)   18496       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 20, 20, 64)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 20, 64)   256         re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 10, 10, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 10, 10, 64)   0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 128)    73856       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 8, 8, 128)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 128)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 4, 4, 128)    0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2048)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 28)           57372       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 28)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 28)           112         re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 28)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 28)           812         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 28)           0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 194,024\n",
      "Trainable params: 193,320\n",
      "Non-trainable params: 704\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape):\n",
    "\n",
    "    dropRate = 0.25\n",
    "    \n",
    "    init = Input(input_shape)\n",
    "    x = BatchNormalization(axis=-1)(init)\n",
    "    x = Conv2D(8, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(8, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(16, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    c1 = Conv2D(16, (3, 3), padding='same')(x)\n",
    "    c1 = ReLU()(c1)\n",
    "    c2 = Conv2D(16, (5, 5), padding='same')(x)\n",
    "    c2 = ReLU()(c2)\n",
    "    c3 = Conv2D(16, (7, 7), padding='same')(x)\n",
    "    c3 = ReLU()(c3)\n",
    "    c4 = Conv2D(16, (1, 1), padding='same')(x)\n",
    "    c4 = ReLU()(c4)\n",
    "    x = Concatenate()([c1, c2, c3, c4])\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    x = Conv2D(32, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    #x = Conv2D(256, (1, 1), activation='relu')(x)\n",
    "    #x = BatchNormalization(axis=-1)(x)\n",
    "    #x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    #x = Dropout(0.25)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(28)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(28)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    model = Model(init, x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# unit test\n",
    "model = create_model(SHAPE)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-03),\n",
    "    metrics=['acc',f1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "specimen_ids, labels = get_train_data(TRAIN_PATH, LABEL_PATH)\n",
    "train_set_sids = specimen_ids[0:3]\n",
    "train_set_lbls = labels[0:3]\n",
    "val_set_sids = specimen_ids[4:]\n",
    "val_set_lbls = labels[4:]\n",
    "\n",
    "# create data generators\n",
    "tg = HproteinDataGenerator(train_set_sids, train_set_lbls)\n",
    "vg = HproteinDataGenerator(val_set_sids, val_set_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('./base.model', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "reduceLROnPlato = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "(192, 192, 4)\n",
      "(192, 192, 4)\n",
      "(192, 192, 4)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_stacked_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7506bb299fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m/Users/farrar/py3.6.5/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farrar/py3.6.5/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farrar/py3.6.5/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farrar/py3.6.5/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farrar/py3.6.5/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farrar/py3.6.5/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/farrar/py3.6.5/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(uid, i)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mi\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2a5f057f263e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecimen_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecimen_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mfeature_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stacked_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecimen_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# augment images if desired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_stacked_image' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "use_multiprocessing = False # DO NOT COMBINE MULTIPROCESSING WITH CACHE! \n",
    "workers = 1 # DO NOT COMBINE MULTIPROCESSING WITH CACHE! \n",
    "\n",
    "hist = model.fit_generator(\n",
    "    tg,\n",
    "    steps_per_epoch=len(tg),\n",
    "    validation_data=vg,\n",
    "    validation_steps=8,\n",
    "    epochs=epochs,\n",
    "    use_multiprocessing=use_multiprocessing,\n",
    "    workers=workers,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
